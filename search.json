[
  {
    "objectID": "portfolio/welcome/index.html",
    "href": "portfolio/welcome/index.html",
    "title": "Welcome To My Portfolio",
    "section": "",
    "text": "This is the first article in my portfolio. Welcome!"
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first article in my blog. Welcome!"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Mavi, PhD",
    "section": "",
    "text": "Welcome To My Portfolio\n\n\n\nNews\n\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "includes/estimator_HT.html",
    "href": "includes/estimator_HT.html",
    "title": "Mavi, PhD",
    "section": "",
    "text": "Let $ = {1,2,…,N} $ be a finite population. For each \\(i\\in\\mathcal{K}\\), let \\(Y_i\\) be an observable of unit \\(i\\). Our goal is to estimate \\[\nY = \\sum_{i=1}^N Y_i\n\\] the total of interest.\nTo construct the estimator we take a random sample from \\(\\mathcal{K}\\). For \\(n &lt; N\\) let \\(\\bf{S} = \\{i_1,...,i_n\\} \\subset \\mathcal{S}\\) be a random sample. Let \\(\\pi_i\\) be the probability $i \\bf{S}\\(.\nThe variance of Horvitz Thompson is then\\)$ Var(Y) = {i=1}N{j=1;ji}^{N} (ip_j - p{ij})( - )^2 \\[\n\\] Var(mu) = {i=1}N{j=1;ji}^{N} (ip_j - p{ij})( - )^2 $$\nIt is not hard to see the variance has an upper bound, \\[\nVar(\\hat Y) \\leq \\sum_{i=1}^n \\frac{Y_i^2}{\\pi_i}\n\\] but if some of the \\(\\pi_i\\) are small this raises the small denominator problem. So care would have to be taken in this case. This danger can be mollified through a modification of the HTE, the Hajek estimator.\nOn the other hand, the above gives an upper bound on the estimate of the mean \\[\nVar(\\hat mu) \\leq \\frac{1}{N^2} \\sum_{i=1}^n \\frac{Y_i^2}{\\pi_i}\n\\]\n\nExample … An example of the HTE is useful. Consider a collection of cities with populations greater than some lower bound \\(b\\) in some state. Let the cities be enumerated by \\(\\mathcal{K} =  \\{1,2,..,N\\}\\). Let \\(Y_i\\) be the total number of hotels in the ith city.\n\nLet \\(\\pi_i\\) be proportional to the population of the ith city. \\[\n\\pi_i = \\frac{q_i}{\\sum_{i=1}^N q_i}\n\\] where \\(q_i\\) is the population of the ith city. The estimate of the average number of hotels in large cities would be given by \\(\\hat mu\\) after sampling \\(n\\) cities. \\[\n\\hat mu = \\frac{1}{n}\\sum_{i\\in \\bf{S}} \\frac{Y_i}{\\pi_i}\n= (\\sum_{i=1}^N q_i) \\left( \\frac{1}{n} \\sum_{i \\in \\bf{S}} \\frac{Y_i}{q_i} \\right)\n\\] the summand can be seen to be the average number of hotels by population, so the average of averages gives an estimate of the hotel density. The prefactor of the total population scales the estimate up to the population. Thus we have our estimate for the total hotels in large cities in our state."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Mavi, PhD",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\nNews\n\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rajinder Mavi, PhD",
    "section": "",
    "text": "Welcome!\n\n\n\nUnder Construction"
  },
  {
    "objectID": "includes/estimator.html",
    "href": "includes/estimator.html",
    "title": "Mavi, PhD",
    "section": "",
    "text": "In the language of statistics, an esimator is a formula or rule employed to estimate a quantity of interest based on observed data. An estimator is said to be unbiased if it’s expectation is equal to the quantity of interest.\nFamiliar examples include the Expectation and Variance estimators for a set of observations \\((X_1,..,X_n)\\) of a random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n\nExpectation\n\n\\[\n\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\] The estimator is unbiased since \\[\n\\mathbb{E} [\\bar X_n] = \\mu\n\\]\nIt is useful to find (or at least bound above) the variance of the estimator, since that guarantees a good estimate if it can be shown to be small. The variance is \\[\nVar(\\bar X_n)  = \\mathbb{E}[(\\bar X_n - \\mathbb{E}[\\bar_n])^2] = \\frac{1}{n}\\sigma^2\n\\] so the variance tends to zero with a large sample, assuring a quality estimator that is unbiased and converges to the quantity of interest.\n\nVariance\n\n\\[\nS_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar X_n)^2\n\\] \\[\n\\mathbb{E} [S_n^2] = \\sigma^2\n\\]\nThe variance is somewhat more complicated, \\[\nVar(S^2_n) = \\frac1n \\left( \\mathbb{E}[(X - \\mu)^4] -\\frac{n-3}{n-1}\\sigma^4 \\right)\n\\] nevertheless still tends to zero with a large sample.\nNote we did not use the estimator \\[\n\\tilde S_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar X_n)^2\n\\] as this woule have expectation \\[\n\\mathbb{E} [\\tilde S_n^2] = \\frac{n}{n-1}\\sigma^2\n\\] and would be a biased estimator."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site …"
  }
]