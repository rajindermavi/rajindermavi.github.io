[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Mavi, PhD",
    "section": "",
    "text": "LinkedIn Messaging Feature Proposal\n\n\n\nData Science\n\n\n\n\n\n\n\n\n\nDec 1, 2025\n\n\nRajinder Mavi\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Mavi, PhD",
    "section": "",
    "text": "Causal Inference With Interference\n\n\n\nAB Testing\n\nData Science\n\nStatistics\n\nAcademic\n\n\n\n\n\n\n\n\n\nDec 1, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rajinder Mavi, PhD",
    "section": "",
    "text": "Welcome!\n\n\n\nUnder Construction"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site …"
  },
  {
    "objectID": "blog/causal_inference_with_interference/index.html",
    "href": "blog/causal_inference_with_interference/index.html",
    "title": "Causal Inference With Interference",
    "section": "",
    "text": "In a causal experiment we would like to measure the impact of a treatment on a quantity of interest. To fix ideas, suppose we have a social media site and are considering an update on the messaging function. The goal might be to increase user messaging frequency or even to improve the quality of messages as measured by the overall length of each message. In the classical sense, there is no way to practically isolate users in a way that would satisfy the SUTVA assumption, hence the need to update the theoretical foundations causal experiments are based upon.\nClassic causal testing typically relies on an assumption of no interference between individuals (for example, the SUTVA framework). With the no-interference assumption, the outcomes of each individual are unaffected by the treatment of each other individual.\nSocial media created a need for causal testing theory and methods that flips that assumptions and conversely handles the case of dense interactions between individuals. From the perspective of the classical theory there would be no guarantee such methodolody would exist, which makes the discoveries of the early social media days a remarkable story.\nBackground for this note includes the following …\n\n\n\n\n\n\nNoteEstimator\n\n\n\n\n\nIn the language of statistics, an esimator is a formula or rule employed to estimate a quantity of interest based on observed data. An estimator is said to be unbiased if it’s expectation is equal to the quantity of interest.\nFamiliar examples include the Expectation and Variance estimators for a set of observations \\((X_1,..,X_n)\\) of a random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nExpectation Estimator\n\\[\n\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\] The estimator is unbiased since \\[\n\\mathbb{E} [\\bar X_n] = \\mu\n\\]\nIt is useful to find (or at least bound above) the variance of the estimator, since that guarantees a good estimate if it can be shown to be small. The variance is \\[\nVar(\\bar X_n)  = \\mathbb{E}[(\\bar X_n - \\mathbb{E}[\\bar X_n])^2] = \\frac{1}{n}\\sigma^2\n\\] so the variance tends to zero with a large sample, assuring a quality estimator that is unbiased and converges to the quantity of interest.\nVariance Estimator\n\\[\nS_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar X_n)^2\n\\] \\[\n\\mathbb{E} [S_n^2] = \\sigma^2\n\\]\nThe variance is somewhat more complicated, \\[\nVar(S^2_n) = \\frac1n \\left( \\mathbb{E}[(X - \\mu)^4] -\\frac{n-3}{n-1}\\sigma^4 \\right)\n\\] nevertheless still tends to zero with a large sample.\nNote we did not use the estimator \\[\n\\tilde S_n^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar X_n)^2\n\\] as this woule have expectation \\[\n\\mathbb{E} [\\tilde S_n^2] = \\frac{n-1}{n}\\sigma^2\n\\] and would be a biased estimator.\n\n\n\n\n\n\n\n\n\nNoteHorvitz Thompson Estimator\n\n\n\n\n\nLet $ = {1,2,…,N} $ be a finite population. For each \\(i\\in\\mathcal{K}\\), let \\(Y_i\\) be an observable of unit \\(i\\). Our goal is to estimate \\[\nY = \\sum_{i=1}^N Y_i\n\\] the total of interest. And \\[\n\\tau = \\frac{1}{N}  \\sum_{i=1}^N Y_i\n\\] the average of the population.\nTo construct the estimator we take a random sample of \\(n&lt; N\\) units from \\(\\mathcal{K}\\). Any sampling method is allowed, but the obtained sample must have distinct units. Let \\(\\pi_i\\) be the probability the \\(i^{th}\\) unit is included in the sample (assume \\(\\pi_i &gt; 0\\)). Denote the random sample by \\(\\bf{S} = \\{i_1,...,i_n\\} \\subset \\mathcal{S}\\). The Horvitz Thompson esimator is \\[\n\\hat Y_{HT} = \\sum_{i\\in \\bf{S}} \\frac{Y_i}{\\pi_i} = \\sum_{i=1}^N \\frac{Y_i 1_{\\{i\\in \\bf S\\}}}{\\pi_i}\n\\] and \\[\n\\hat \\tau_{HT} = \\frac{1}{N} \\hat Y_{HT}\n\\]\nThe estimator is unbiased \\[\n    \\mathbb{E}[\\hat Y_{HT}] = Y.\n\\] \\[\n    \\mathbb{E}[\\hat \\tau_{HT}] = \\tau.\n\\]\nTo state the variance of \\(\\hat Y_{HT}\\), define \\(\\pi_{ij}\\) as the probability that both \\(i\\) and \\(j\\) belong to \\(\\bf{S}\\). The variance of Horvitz Thompson is then \\[\nVar(\\hat Y_{HT})\n    =\n        \\sum_{i=1}^N \\sum_{j=1}^{N} \\frac{\\pi_{ij} - \\pi_i\\pi_j}{\\pi_i\\pi_j} Y_i Y_j\n\\] it follows from the definition that \\(\\pi_{ii}\\) = \\(\\pi_i\\). The variance of tha average estimator is \\[\nVar(\\hat \\tau_{HT})\n    =\n        \\frac{1}{N^2}\\sum_{i=1}^N \\sum_{j=1}^{N} \\frac{\\pi_{ij} - \\pi_i\\pi_j}{\\pi_i\\pi_j} Y_i Y_j\n\\]\nAs we want to assure that the Horwitz Thompson estimator converges well to the quantity of interest, controlling the variance is key. It is not hard to see the variance has an upper bound, \\[\nVar(\\hat Y_{HT}) \\leq \\sum_{i,j=1}^N \\left(\\frac{1}{\\pi_i^{1/2}\\pi_j^{1/2}} - 1\\right) Y_i Y_j\n\\] but if some of the \\(\\pi_i\\) are small this raises the small denominator problem. So care would have to be taken in this case. This danger can be mollified through a modification of the HTE, the Hajek estimator. (Also note that as the sample becomes larger, the \\(\\pi_i\\) become larger, better controlling the variance.)\nExample\nConsider a collection of cities with populations greater than some lower bound \\(b\\) in some state. Let the cities be enumerated by \\(\\mathcal{K} =  \\{1,2,..,N\\}\\). Let \\(Y_i\\) be the total number of hotels in the ith city.\nSample \\(n\\) units from \\(\\mathcal{K}\\) by Simple Random Sampling Without Replacement. The inclusion probabilities are \\[\n\\pi_i = n/N\n\\] \\[\n\\pi_{ij} = \\frac{n(n-1)}{N(N-1)}\n\\]\nThe estimate of the average number of hotels in large cities would be given by \\(\\hat Y_{HT}\\) after sampling \\[\n\\hat Y_{HT} = \\sum_{i\\in \\bf{S}} \\frac{Y_i}{\\pi_i}\n= \\frac{N}{n}\\sum_{i\\in \\bf{S}} Y_i\n\\] with variance \\[\nVar(\\hat Y_{HT}) = N^2 \\frac{1 - n/N}{n} S^2\n\\] where \\(S^2\\) is the population variance. The variance does not tend to zero even as m tends to infinity. But, for the Horwitz Thompson average, \\[\nVar(\\hat \\tau_{HT}) = \\frac{1 - n/N}{n} S^2\n\\] Making a simple assumption \\(n \\propto N^\\alpha\\) for \\(0 &lt; \\alpha &lt; 1\\) or \\(n = \\beta N\\) for \\(0 &lt; \\beta &lt; 1\\), the variance of the estimator of the average tends to zero.\n\n\n\nEnumerate the users \\(\\mathcal{V} = \\{v_1,v_2,...,v_N\\}\\). Typically, the network is modeled as a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\). In our example two units may be connected by an edge if they are freinds. For the sake of the experiment, each user is assigned to one of two groups \\(z_i = 0,1\\), with \\(0\\) being the control and \\(1\\) being the treatment.\nWith an assignment of \\(z = (z_1,z_2,..,z_N)\\) the total of the quantity of interest is \\[\nY(z) = \\sum_{i=1}^N Y_i(z)\n\\] where \\(Y_i\\) is the quantity of interest for the \\(i^{th}\\) individual. For our messaging example, \\(Y_i\\) might be the total number of messages the user sent within a two week period. Notice at this point we are allowing that each individual’s behavior depends on the entire assignment vector!\nThe ulimate goal is to estimate \\[\n\\tau(\\vec{1},\\vec{0}) = \\frac1N\\sum_{i=1}^N \\left(Y_i(\\vec{1}) - Y_i(\\vec{0}) \\right)\n\\tag{1}\\] the Average Treatment Effect (ATE). Here \\(\\vec{1}\\) (\\(\\vec{0}\\)) is the assignment of \\(1\\)s (\\(0\\)s) to all individuals. Of course there is no world where we can simultaneously assign all units both 1 and 0, hence the goal of our study.\nIt would be hard to make it very far without making some kind of assumptions on the network effects. How can we make conclusions about \\(\\tau(\\vec{1},\\vec{0})\\) if we can’t isolate the assignment groups? The earliest paper I found in this direction is (Ugander et al. 2013). The authors make the assumption of a net exposure condition which effectively assumes that if sufficiently many agents of an individuals neighborhood belong to a given assignment group, the individual will behave as if the entire network has that assignment. That is, for indivual \\(v\\in\\mathcal{V}\\), suppose there are \\(k_i\\) individuals in \\(A_{1,r}(v)\\) assigned \\(z_j = 1 (0)\\), then we can assume \\(i\\) behaves as if \\(z = \\vec{1} (\\vec{0})\\). Explicit choices of \\(k_i\\) can vary, but the authors consider \\(k_i\\) the size of the neighborhood \\(k_i = |A_{1,r}(v)|\\), or a proportion of the neighborhood \\(k_i = \\kappa |A_{1,r}(v)|\\), or simply a constant minimum over the network \\(k_i = \\kappa\\).\nThe Ugander paper is notable because they tackle the problem of a globally connected network of individuals. Compare this to the more mature literature of estimating the ATE when there are groups of interacting agents (Hudgens and Halloran 2008). In that paper, the population is stratified into groups, and interference is limited to individuals among the same group. For example, a group might be all students in a particular elementary school. The analogy of the net exposure condition, is the assumption that any two assignments of a fixed group are equivalent if they have the same number of individuals assigned to 1 and 0 respectively.\n\n\n\n\n\n\nNoteHudgens and Halloran, 2008\n\n\n\n\n\nThe Hudgens and Halloran paper outlines detailed experiment designs. [Sampling proceedures] The goal is to measure quantities like (Equation 1) by group and by individual. They differentiate a number of different causal effects * Direct - The impact on \\(Y_i\\) of altering a single individuals assignment \\(z_i=0 \\to z_i=1\\) while holding all other assigments fixed. * Indirect - The impact on \\(Y_i\\) of altering the assignment of all other individuals \\(\\{z_j\\}_{j\\neq i}\\) of a group while holding \\(z_i\\) fixed. * Total - A combination of direct and indirect effects.\nWith this variety of quatities the proceed to derive unbiased estimators for the ATEs. [State corollary of theorem 2]\n\n\n\npapers (Hudgens and Halloran 2008), (Aronow and Samii 2017), (Ugander et al. 2013), (Eckles, Karrer, and Ugander 2017)\n\n\n\n\n\nReferences\n\nAronow, Peter M., and Cyrus Samii. 2017. “Estimating Average Causal Effects Under General Interference, with Application to a Social Network Experiment.” The Annals of Applied Statistics 11 (4): 1912–47. http://www.jstor.org/stable/26362172.\n\n\nEckles, Dean, Brian Karrer, and Johan Ugander. 2017. “Design and Analysis of Experiments in Networks: Reducing Bias from Interference.” Journal of Causal Inference 5 (1): 20150021.\n\n\nHudgens, Michael G., and M. Elizabeth Halloran. 2008. “Toward Causal Inference with Interference.” Journal of the American Statistical Association 103 (482): 832–42. http://www.jstor.org/stable/27640105.\n\n\nUgander, Johan, Brian Karrer, Lars Backstrom, and Jon Kleinberg. 2013. “Graph Cluster Randomization: Network Exposure to Multiple Universes.” In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 329–37. KDD ’13. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2487575.2487695."
  },
  {
    "objectID": "portfolio/linkedin-messaging-feature/index.html",
    "href": "portfolio/linkedin-messaging-feature/index.html",
    "title": "LinkedIn Messaging Feature Proposal",
    "section": "",
    "text": "Let’s talk about A/B testing.\nSpecifically, we’ll consider an enhancement to their direct messaging service. Check out this proposal by Dan Vang.\nEssentially, the idea is to insert a field where a user can privately add a short blurb  to contextualize their connection to another user."
  },
  {
    "objectID": "portfolio/linkedin-messaging-feature/index.html#linkedins-process",
    "href": "portfolio/linkedin-messaging-feature/index.html#linkedins-process",
    "title": "LinkedIn Messaging Feature Proposal",
    "section": "LinkedIn’s Process",
    "text": "LinkedIn’s Process\nTypically rolling out a new feature on any social networking site requires collecting metrics to demonstrate it’s impact and utility for users. In this article we will run through this vetting process.\nLinkedIn has a robust A/B testing methodology developed over years as the world’s leading professional networking site. This 2015 article (Xu et al. 2015) details early scaling solutions. And this (Ivaniuk and Duan 2020b) pair (Ivaniuk and Duan 2020a) of 2020 bog posts lay out their mature infrastructure and methodology for A/B testing, called LinkedIn Targeting, Ramping, and Experimentation platform, or T-REX. All this preparation yields impressive scaling capabilities, supporting 2,000 new experiments per week (as of the 2020 post)."
  },
  {
    "objectID": "portfolio/linkedin-messaging-feature/index.html#testing-considerations",
    "href": "portfolio/linkedin-messaging-feature/index.html#testing-considerations",
    "title": "LinkedIn Messaging Feature Proposal",
    "section": "Testing Considerations",
    "text": "Testing Considerations\nThere are a number of considerations to address when designing a test. We’ll break them down into user segmentation and clustering, test conflicts, and timeline.\n\nClustering and Segmentation\nThere are many ways users may be segmented on a social platform like LinkedIn. Consider the many user types on the platform:\n\nJob Seekers/Professionals\nRecruiters\nInfluencers\nSales & Marketing Specialists\nNGO/Nonprofit Workers\nEntrepreneurs/Startups\nCorporate Managers\n\nHowever, natural clusters may not respect these nice categories. Managers and professionals within the same company may be highly networked, whereas professionals in different specialties (Materials Science vs Data Science) may be completely disjoint.\nThe definition of `Natural Clusters’ in this context may not be so straightforward, but any good A/B testing would either assert user independence (IE Stable Unit Treatment Value Assumption (SUTVA)) or find a methodology to address that issue.\nSince our feature relates to user interactions, it would be wise to consider user clusters as the unit users would be randomized by and assigned treatment groups. This strategy is designed to limit the event of spillover from one treatment group to another. With this approach we must be careful in our estimation of parameters as user selection into treatment groups is affected. Users placed in treatment groups remains a random variable with the same probability distribution, but those random variables are no longer independent of each other.\n\n\n\nConflicts\n\nAlthough LinkedIn allows for overlapping experiments, we would like to avoid intersecting with other experiments on messaging to avoid confounding factors.\nDisjoint:\n\nDirect Messaging Service attributes. EG: DM notifications, look and feel updates, LinkedIn inmail sponsored messages.\n\nIndependent:\n\nNon Direct Messaging Service features. Other notifications, feed updates, etc.\n\nChanging browser types. Every effort should be made to maintain a smooth operating experience between different browsers, however, users switching browsers could be a complicating factor. It could also provide valuable information on which formats provide the best outcomes.\n\n\n\nTimeline\nWe should include a reasonable burn-in period for a novel feature such as this.\nTypical A/B testing cycles run on a timeline of a minimum of 2 weeks to a maximum of 6 - 8 weeks. We will assume a round 8 weeks.\nIdeally we would like to know the impact over a longer timeline. However, for business and experimental control reasons we would like to conclude on a quicker timeline, thus we can use adoption as a proxy on the assumption that memory aids will improve engagements between loosely connected members."
  },
  {
    "objectID": "portfolio/linkedin-messaging-feature/index.html#kpis-and-statistics",
    "href": "portfolio/linkedin-messaging-feature/index.html#kpis-and-statistics",
    "title": "LinkedIn Messaging Feature Proposal",
    "section": "KPIs and Statistics",
    "text": "KPIs and Statistics\nNow time for the fun stuff.\nThere are many possible KPIs we could utilize for the above aspects we would like to measure. For ease of exposition, we will suggest simple KPIs for each of the above.\n\nUtilization KPIs\n\nAdoption.\n\nTracking users who use the feature when it becomes available to them, a higher adoption rate suggests more users find it relevant.\n\n\n\\[\nA = \\frac{\\#\\text{users who used the feature at least once}}{\\#\\text{users exposed to the feature}}\n\\]\n\nFrequency.\n\nMeasure total number of uses over the course of the trial. Higher frequency of use implies each user finds value in the feature over many connections. As use of the feature is dependent on entering the messaging window, an opportunity would be defined as entering a messaging window where the summary is not yet filled in. \\[\nF = \\frac{\\#\\text{ times feature is used.}}{\\#\\text{opportunities to use the feature.}}\n\\]\n\nRetention.\n\nCheck that users return to the feature over a specified time span. This indicates the user found value for the feature in the past and creates a stickier environment for the user. As we will observe just over the course of several weeks, we will test with a 7 day retention window. \\[\nR = \\frac{\\#\\text{users who used the feature 7 days after their first use.}}{\\#\\text{users who used the feature.}}\n\\]\n\n\n\n\nA/B testing KPIs:\n\nDirect Messages\n\nAs our feature lives in the direct message service, this would be the primary effect we would like to investigate. \\[\n  M = \\frac{\\#\\text{Direct Messages}}{\\#\\text{Users with context.}}\n  \\]\n\nEngagement\n\nA secondary effect of providing context to peers is a change in engagement, measured in likes, comments, and shares. \\[\n  E = \\frac{\\#\\text{Likes + Comments + Shares}}{\\#\\text{Impressions}}\n  \\]"
  }
]